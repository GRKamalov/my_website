---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-20"
description: My projects in Data Science and Machine Learning # the title that will show up once someone gets to this page
draft: false
#image: spices.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: ds # slug is the shorthand URL address... no spaces plz
title: My projects
---

What's it all about?
========================

Apart from my data endeavours at London Business School, I received some training in Data Science and Machine Learning during my undergraduate years. Here you will see some of my efforts related to the prominent [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) textbook by Hastie, Tibshirani et al. Here I will present some examples of my work, plot reproductions and ML algorithm implementations, including `boosting,` `LOESS` and many more.


LASSO - evolution of coefficients with the penalty constant
========================

Lasso is a very powerful tool for shrinkage of regression models with many explanatory variables. The algorithm performs the tasks of *variable selection* and *dimension reduction* for the set of predictors, yielding sparse models with high interpretability. Mathematically, LASSO is in fact a convex optimization proble, summarized as follows:

$$\hat{\beta}^{LASSO} = argmin_\beta \bigg\{0.5\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|\bigg\}$$ 
So, let's look how the dimension reduction works on practice, once we try to adjust the shrinkage parameter $\lambda$!
```{r, message = FALSE, warning = FALSE, fig.height = 8, fig.width = 8}
rm(list=ls())
library(data.table)
library(glmpath)
library(latex2exp)
data(heart.data)
print("I have no idea how to hide the console output which keeps appearing below this line. Yes, I tried the \014 command...")
attach(heart.data)
cat("\014")

data_x <-heart.data$x
data_x <-data_x[,-6]
data_x <-data_x[,-4]
data_y <- heart.data$y
reg <- glmpath(x=data_x,y=data_y,family=binomial)
par(mar = c(5, 5, 2, 5),col.lab="white", cex = 0.9)
plot.glmpath(reg, type="coefficients",xlimit=2.35,col=c("black","orange","grey","green","brown","red","blue"),main="",font=8)
title(cex.lab=0.001)
title(xlab=TeX('$|\\beta(\\lambda)|$'),ylab=TeX('Coefficients $\\beta_j(\\lambda)$'),col.lab="black")
cat("\014")

```
##Nonparametric Kernel regressions

Using linear regression, we implicitly assume that marginal effects of our predictors are constant - at least, without feature engineering? But what if they are not? And, more broadly, what if we allow the marginal effects of predictors to be free from a particular functional form? Thus, we arrive at **nonparametric Kernel regressions**.

Here, you can see the marginal effects of various predictors on the level of atmospheric ozone - quite a scientific task. Especially, we can see how these marginal effects *change* with the evolution of other predictors. So, let's have a look: 

```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(data.table)
library(np)
library(latex2exp)
library(lattice)


rm(list = ls())
library(lattice)
set.seed(1)
data <- fread("http://web.stanford.edu/~oleg2/hse/ozone/ozone.data")
data$cubicroot <- (data$ozone)**(1/3)

data$Temp <- equal.count(data$temperature, number=4, overlap=0.15)
data$Wind <- equal.count(data$wind, number=4, overlap=0.15)
par(cex=0.5)
plt <- xyplot(cubicroot ~ radiation | Temp*Wind, data = data,
              xlab = "Solar radiation (langleys)",
              ylab = "Cube root ozon (cube root ppb)",
              panel = function(x, y) {
                 panel.grid(h =- 1, v = 2)
                   localreg <- npreg(y~x,regtype="ll",gradients=TRUE)
                   newdata = data.table(x=x,y=predict(localreg))
                   newdata <- newdata[order(x)]
                   panel.lines(newdata$x,newdata$y)
              },drop.unused.levels = TRUE
)
update(plt, par.settings = list(cex=5,fontsize = list(text =8, points = 12)))
```

How to perform variable selection?
=================================

Here is an example of a very interesting dataset, concerned with decoding of phonemes (i.e. signal processsing). Each signal is discretized (i.e., there are a number of discrete points out of the whole transmission), and a spline basis of variable complexity is used to restore the original signal. The question of choosing the optimal cardinality of basis function set shows us a very good example of **bias-variance trade-off**. At first, increased complexity gives us enhanced precision, but afterwards something very opposite starts to happen... So, let's have a look together!

```{r, message = FALSE, warning = FALSE, results = 'hide'}
rm(list=ls())
library(data.table)
library(splines)
library(latex2exp)
#Since the dataset is in a .zip archive, I download it to the local machine and then read it from the directory.
set.seed(125)
data <- fread("phoneme.data")
              
#prepare and split the data
data <- data[g == "aa" | g == "ao"]
data[, speaker := NULL]
data[,row.names := NULL]
data[, y := ifelse(g == "aa", 1,0)]
data[, g := NULL]
y_aux <- cbind(data[,y])
data[, (colnames(data)) := lapply(.SD, scale), .SDcols=colnames(data)]
data[,y := NULL]
data[,y := y_aux]
train_sample <- sample(1:dim(data)[1],size = 1000)
data_train <- data[train_sample]
data_test <- data[-train_sample]

#proceed with the modelling
metrics <- data.table(aiclog =numeric(),llic = numeric(),llic_test = numeric(),aicbin = numeric(),lbin = numeric(),lbin_test = numeric())
n_basis <- c(2,4,8,16,32,64,128,256)
spline_grid <- seq(1,256,by = 1)
y_train <- cbind(data_train$y)
data_train[,y := NULL]
y_test <- cbind(data_test$y)
data_test[,y := NULL]
for (item in n_basis){

  spline_basis <- ns(spline_grid, df = item+2, intercept = TRUE)
  data_fitted_train <- as.matrix(data_train)  %*% as.matrix(spline_basis)
  data_fitted_train <- data.table(data_fitted_train)
  data_fitted_train[,y := y_train]
  
  data_fitted_test <- as.matrix(data_test)  %*% as.matrix(spline_basis)
  data_fitted_test <- data.table(data_fitted_test)
  data_fitted_test[,y := y_test]
  
  model <- glm(y~., data = data_fitted_train, family = binomial)
  
  #calculate the metrics for the left plot:
  test <- predict(model, newdata = data_fitted_test, type = "response")
  test_calc_log <- data.table(test = test, y = y_test)
  llic_train <- model$deviance/1000
  aic_fitted <- model$aic/1000
  llic_test <- -2*sum(test_calc_log$y*log(test_calc_log$test)+(1-test_calc_log$y)*log(1-test))/(dim(data_test)[1])

  #do some data processing and calculate the metrics for the right plot:
  data_fitted_train[,y_fitted := predict(model, type = "response")]
  data_fitted_train[,y_fitted_10 := mapply(function(x){ifelse(x>=0.5,1,0)},x=y_fitted)]
  binary_loss_train <- sum(abs(data_fitted_train[,y]-data_fitted_train[,y_fitted_10]))/1000
  
  data_fitted_test[,y_fitted := predict(model, newdata=data_fitted_test,type = "response")]
  data_fitted_test[,y_fitted_10 := mapply(function(x){ifelse(x>=0.5,1,0)},x=y_fitted)]
  binary_loss_test <- sum(abs(data_fitted_test[,y]-data_fitted_test[,y_fitted_10]))/(dim(data_fitted_test)[1])
  
  binary_aic <- binary_loss_train + length(model$coefficients)/(dim(data_fitted_train)[1])
  
  metrics <- rbind(metrics, list(aic_fitted,llic_train,llic_test,binary_aic,binary_loss_train,binary_loss_test), use.names = FALSE)
}


metrics[,count := c(1,2,3,4,5,6,7,8)]

#do the plotting:


par(mfrow=c(1,2))
plot(metrics$count, metrics$aiclog, "o", col="darkgreen",main=TeX('Log-likelihood loss'),xlab=TeX('Number of basis functions'),ylab=TeX('Log-likelihood'),ylim=c(0.35,2.65),xaxt="n")
axis(side=1, at=c(1,2,3,4,5,6,7,8), labels = FALSE)

text(x=c(1,2,3,4,5,6,7,8),  par("usr")[3], 
     labels = n_basis, srt = 0, pos = 1, xpd = TRUE)
lines(metrics$count,metrics$llic,"o", col = "orange")

lines(metrics$count,metrics$llic_test,"o",col="deepskyblue")
legend("topleft",legend = c("Train","Test","AIC"), col=c("orange","deepskyblue","darkgreen"),lty=1,lwd=1)


plot(metrics$count,metrics$aicbin,"o",col="darkgreen",main=TeX('0-1 Loss'),xlab=TeX('Number of basis functions'),ylab=TeX('Misclassification Error'),ylim=c(0.075,0.365),xaxt="n")
axis(side=1, at=c(1,2,3,4,5,6,7,8), labels = FALSE)

text(x=c(1,2,3,4,5,6,7,8),  par("usr")[3], 
     labels = n_basis, srt = 0, pos = 1, xpd = TRUE)
lines(metrics$count,metrics$lbin,"o", col = "orange")

lines(metrics$count,metrics$lbin_test,"o",col="deepskyblue")


```

Boosting: how many iterations are required?
==========================================

We all know that boosting is an iterative algorithm which steps from the original data to residuals of the previous step, and continues on an on. It is quite obvious that the positive effect of increasing model complexity, hence, should be diminishing. Indeed, there is less and less of meaningful information left in the data, and more noise starts to appear. But how does it all look in a graph? 
Here, we have an example of **6-fold CV error** of a boosting algorithm used for prediction of some target variable in a simulated dataset:


```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(caret)
library(rpart)
library(data.table)
library(tree)
library(latex2exp)
library(gbm)
set.seed(2)


generate_data <- function(N_total,negative){
   X <- rnorm(N_total*10)
   X <- matrix(data = X, nrow = N_total, ncol = 10)
   data <- as.data.table(X)
   data[,y := ifelse(rowSums(data**2) >= qchisq(p=0.5,df=10),1,negative)]
   data[,index_number := 1:dim(data)[1]]
   return(data)
}
N_total <- 12000
N_trees <- 400
tree_error_corr <- 0.02
data <- generate_data(N_total,0)
data[,index_number := NULL]

N_loocv <- 2001
step_loocv <- (N_loocv-1)/1000
data[, y := as.factor(y)]
#since we have 2000 training observations out of 12000 observations in total, the natural choice would be to set K=6
train_control <- trainControl(method = "cv", number = 6)
model_cv <- train(y~., data = data, method = "gbm", distribution = "adaboost", metric = "Accuracy", trControl = train_control, tuneGrid = expand.grid(n.trees = seq(1,N_loocv,by=step_loocv),interaction.depth=1,shrinkage=0.95,n.minobsinnode=10))
 
vals_cv <- data.table(test_error = 1-model_cv$results$Accuracy, grid = seq(1,N_loocv, by=step_loocv))

plot(vals_cv[grid>=200,grid], vals_cv[grid>=200,test_error], type="l", xlab = TeX("Boosting Iterations"), ylab = TeX("6-fold CV estimate of test error"), col = "orange", main = TeX("6-fold CV estimate of test error"))
text(850,0.040,"Minimum point of CV-estimated test error", pos=3, cex=0.85)
 
abline(v=vals_cv[test_error==min(test_error),grid],lty="dotted")
abline(h=vals_cv[test_error==min(test_error),test_error],lty="dotted")
```


So, minimum CV error is attained for the following number of parameters of `boosting` algorithm:
```{r}
print(c(vals_cv[test_error==min(test_error),grid],"iterations"))
```